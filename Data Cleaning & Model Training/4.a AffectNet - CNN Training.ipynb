{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0939ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# !pip install librosa\n",
    "# !pip install lifelines\n",
    "# !pip install np_utils\n",
    "# !pip install tensorflow\n",
    "\n",
    "import librosa\n",
    "import librosa.display\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import keras\n",
    "from keras.callbacks import ReduceLROnPlateau\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Conv1D, MaxPooling1D, Flatten, Dropout, BatchNormalization\n",
    "from keras.utils import to_categorical\n",
    "from keras.callbacks import ModelCheckpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256044bf",
   "metadata": {},
   "source": [
    "## Importing and Splitting the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea1edbee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#importing the datasets\n",
    "train_set = pd.read_csv('outputs/train_set_ov2_no_mismatch.csv')\n",
    "test_set = pd.read_csv('outputs/test_set_ov2_no_mismatch.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b04bb23f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into features and target; x = features, y = target\n",
    "x_train = train_set.iloc[:, 1:]\n",
    "y_train = train_set['label']\n",
    "\n",
    "x_test = test_set.iloc[:, 1:]\n",
    "y_test = test_set['label']\n",
    "\n",
    "#combining test and train set for cross validation\n",
    "x = pd.concat([x_train, x_test])\n",
    "y = pd.concat([y_train, y_test])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d60a1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lm_1_x</th>\n",
       "      <th>lm_1_y</th>\n",
       "      <th>lm_1_z</th>\n",
       "      <th>lm_2_x</th>\n",
       "      <th>lm_2_y</th>\n",
       "      <th>lm_2_z</th>\n",
       "      <th>lm_3_x</th>\n",
       "      <th>lm_3_y</th>\n",
       "      <th>lm_3_z</th>\n",
       "      <th>lm_4_x</th>\n",
       "      <th>...</th>\n",
       "      <th>lm_475_z</th>\n",
       "      <th>lm_476_x</th>\n",
       "      <th>lm_476_y</th>\n",
       "      <th>lm_476_z</th>\n",
       "      <th>lm_477_x</th>\n",
       "      <th>lm_477_y</th>\n",
       "      <th>lm_477_z</th>\n",
       "      <th>lm_478_x</th>\n",
       "      <th>lm_478_y</th>\n",
       "      <th>lm_478_z</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.491400</td>\n",
       "      <td>0.732320</td>\n",
       "      <td>-0.054371</td>\n",
       "      <td>0.485835</td>\n",
       "      <td>0.661711</td>\n",
       "      <td>-0.142432</td>\n",
       "      <td>0.489872</td>\n",
       "      <td>0.676943</td>\n",
       "      <td>-0.072369</td>\n",
       "      <td>0.467857</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.012128</td>\n",
       "      <td>0.626934</td>\n",
       "      <td>0.445669</td>\n",
       "      <td>-0.012127</td>\n",
       "      <td>0.600117</td>\n",
       "      <td>0.469423</td>\n",
       "      <td>-0.012155</td>\n",
       "      <td>0.626503</td>\n",
       "      <td>0.492920</td>\n",
       "      <td>-0.012152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515338</td>\n",
       "      <td>0.618809</td>\n",
       "      <td>-0.027641</td>\n",
       "      <td>0.520669</td>\n",
       "      <td>0.569473</td>\n",
       "      <td>-0.110649</td>\n",
       "      <td>0.516733</td>\n",
       "      <td>0.578083</td>\n",
       "      <td>-0.047346</td>\n",
       "      <td>0.495328</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.018651</td>\n",
       "      <td>0.630938</td>\n",
       "      <td>0.368347</td>\n",
       "      <td>-0.018649</td>\n",
       "      <td>0.613579</td>\n",
       "      <td>0.387337</td>\n",
       "      <td>-0.018672</td>\n",
       "      <td>0.633811</td>\n",
       "      <td>0.402824</td>\n",
       "      <td>-0.018671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.510069</td>\n",
       "      <td>0.679696</td>\n",
       "      <td>-0.071083</td>\n",
       "      <td>0.520436</td>\n",
       "      <td>0.597828</td>\n",
       "      <td>-0.137022</td>\n",
       "      <td>0.514391</td>\n",
       "      <td>0.623089</td>\n",
       "      <td>-0.075861</td>\n",
       "      <td>0.491742</td>\n",
       "      <td>...</td>\n",
       "      <td>0.034499</td>\n",
       "      <td>0.614713</td>\n",
       "      <td>0.447929</td>\n",
       "      <td>0.034502</td>\n",
       "      <td>0.589599</td>\n",
       "      <td>0.468398</td>\n",
       "      <td>0.034472</td>\n",
       "      <td>0.615148</td>\n",
       "      <td>0.488124</td>\n",
       "      <td>0.034479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.504052</td>\n",
       "      <td>0.716384</td>\n",
       "      <td>-0.037302</td>\n",
       "      <td>0.508079</td>\n",
       "      <td>0.656141</td>\n",
       "      <td>-0.126209</td>\n",
       "      <td>0.504914</td>\n",
       "      <td>0.671283</td>\n",
       "      <td>-0.056388</td>\n",
       "      <td>0.484088</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.005355</td>\n",
       "      <td>0.617545</td>\n",
       "      <td>0.446876</td>\n",
       "      <td>-0.005354</td>\n",
       "      <td>0.593047</td>\n",
       "      <td>0.469114</td>\n",
       "      <td>-0.005385</td>\n",
       "      <td>0.618375</td>\n",
       "      <td>0.489911</td>\n",
       "      <td>-0.005381</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.510106</td>\n",
       "      <td>0.705631</td>\n",
       "      <td>-0.089607</td>\n",
       "      <td>0.501306</td>\n",
       "      <td>0.596600</td>\n",
       "      <td>-0.154930</td>\n",
       "      <td>0.504797</td>\n",
       "      <td>0.633559</td>\n",
       "      <td>-0.084937</td>\n",
       "      <td>0.473188</td>\n",
       "      <td>...</td>\n",
       "      <td>0.027544</td>\n",
       "      <td>0.642669</td>\n",
       "      <td>0.397099</td>\n",
       "      <td>0.027547</td>\n",
       "      <td>0.614630</td>\n",
       "      <td>0.422473</td>\n",
       "      <td>0.027513</td>\n",
       "      <td>0.642357</td>\n",
       "      <td>0.448079</td>\n",
       "      <td>0.027519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2635</th>\n",
       "      <td>0.482593</td>\n",
       "      <td>0.718526</td>\n",
       "      <td>-0.064106</td>\n",
       "      <td>0.469306</td>\n",
       "      <td>0.640415</td>\n",
       "      <td>-0.131103</td>\n",
       "      <td>0.477639</td>\n",
       "      <td>0.663581</td>\n",
       "      <td>-0.071532</td>\n",
       "      <td>0.455022</td>\n",
       "      <td>...</td>\n",
       "      <td>0.004841</td>\n",
       "      <td>0.630023</td>\n",
       "      <td>0.445232</td>\n",
       "      <td>0.004842</td>\n",
       "      <td>0.604482</td>\n",
       "      <td>0.466323</td>\n",
       "      <td>0.004816</td>\n",
       "      <td>0.629380</td>\n",
       "      <td>0.487871</td>\n",
       "      <td>0.004818</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2636</th>\n",
       "      <td>0.478807</td>\n",
       "      <td>0.702516</td>\n",
       "      <td>-0.064206</td>\n",
       "      <td>0.466731</td>\n",
       "      <td>0.617996</td>\n",
       "      <td>-0.130047</td>\n",
       "      <td>0.475330</td>\n",
       "      <td>0.640948</td>\n",
       "      <td>-0.070059</td>\n",
       "      <td>0.453466</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.009315</td>\n",
       "      <td>0.629221</td>\n",
       "      <td>0.444486</td>\n",
       "      <td>-0.009318</td>\n",
       "      <td>0.606545</td>\n",
       "      <td>0.462531</td>\n",
       "      <td>-0.009343</td>\n",
       "      <td>0.627597</td>\n",
       "      <td>0.482280</td>\n",
       "      <td>-0.009339</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2637</th>\n",
       "      <td>0.489660</td>\n",
       "      <td>0.727882</td>\n",
       "      <td>-0.057233</td>\n",
       "      <td>0.491185</td>\n",
       "      <td>0.623539</td>\n",
       "      <td>-0.142210</td>\n",
       "      <td>0.491912</td>\n",
       "      <td>0.646457</td>\n",
       "      <td>-0.066818</td>\n",
       "      <td>0.467872</td>\n",
       "      <td>...</td>\n",
       "      <td>0.003427</td>\n",
       "      <td>0.629216</td>\n",
       "      <td>0.390856</td>\n",
       "      <td>0.003430</td>\n",
       "      <td>0.601153</td>\n",
       "      <td>0.416639</td>\n",
       "      <td>0.003394</td>\n",
       "      <td>0.629049</td>\n",
       "      <td>0.442369</td>\n",
       "      <td>0.003398</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2638</th>\n",
       "      <td>0.499108</td>\n",
       "      <td>0.682507</td>\n",
       "      <td>-0.068565</td>\n",
       "      <td>0.503425</td>\n",
       "      <td>0.585846</td>\n",
       "      <td>-0.135804</td>\n",
       "      <td>0.501222</td>\n",
       "      <td>0.612962</td>\n",
       "      <td>-0.069593</td>\n",
       "      <td>0.482207</td>\n",
       "      <td>...</td>\n",
       "      <td>0.029519</td>\n",
       "      <td>0.633312</td>\n",
       "      <td>0.397255</td>\n",
       "      <td>0.029521</td>\n",
       "      <td>0.608004</td>\n",
       "      <td>0.419239</td>\n",
       "      <td>0.029489</td>\n",
       "      <td>0.632230</td>\n",
       "      <td>0.443043</td>\n",
       "      <td>0.029497</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2639</th>\n",
       "      <td>0.515052</td>\n",
       "      <td>0.727938</td>\n",
       "      <td>-0.044722</td>\n",
       "      <td>0.528304</td>\n",
       "      <td>0.652987</td>\n",
       "      <td>-0.127472</td>\n",
       "      <td>0.519285</td>\n",
       "      <td>0.670686</td>\n",
       "      <td>-0.057918</td>\n",
       "      <td>0.502943</td>\n",
       "      <td>...</td>\n",
       "      <td>0.010694</td>\n",
       "      <td>0.615175</td>\n",
       "      <td>0.439883</td>\n",
       "      <td>0.010695</td>\n",
       "      <td>0.590612</td>\n",
       "      <td>0.463593</td>\n",
       "      <td>0.010662</td>\n",
       "      <td>0.615574</td>\n",
       "      <td>0.485904</td>\n",
       "      <td>0.010667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2640 rows Ã— 1434 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        lm_1_x    lm_1_y    lm_1_z    lm_2_x    lm_2_y    lm_2_z    lm_3_x  \\\n",
       "0     0.491400  0.732320 -0.054371  0.485835  0.661711 -0.142432  0.489872   \n",
       "1     0.515338  0.618809 -0.027641  0.520669  0.569473 -0.110649  0.516733   \n",
       "2     0.510069  0.679696 -0.071083  0.520436  0.597828 -0.137022  0.514391   \n",
       "3     0.504052  0.716384 -0.037302  0.508079  0.656141 -0.126209  0.504914   \n",
       "4     0.510106  0.705631 -0.089607  0.501306  0.596600 -0.154930  0.504797   \n",
       "...        ...       ...       ...       ...       ...       ...       ...   \n",
       "2635  0.482593  0.718526 -0.064106  0.469306  0.640415 -0.131103  0.477639   \n",
       "2636  0.478807  0.702516 -0.064206  0.466731  0.617996 -0.130047  0.475330   \n",
       "2637  0.489660  0.727882 -0.057233  0.491185  0.623539 -0.142210  0.491912   \n",
       "2638  0.499108  0.682507 -0.068565  0.503425  0.585846 -0.135804  0.501222   \n",
       "2639  0.515052  0.727938 -0.044722  0.528304  0.652987 -0.127472  0.519285   \n",
       "\n",
       "        lm_3_y    lm_3_z    lm_4_x  ...  lm_475_z  lm_476_x  lm_476_y  \\\n",
       "0     0.676943 -0.072369  0.467857  ... -0.012128  0.626934  0.445669   \n",
       "1     0.578083 -0.047346  0.495328  ... -0.018651  0.630938  0.368347   \n",
       "2     0.623089 -0.075861  0.491742  ...  0.034499  0.614713  0.447929   \n",
       "3     0.671283 -0.056388  0.484088  ... -0.005355  0.617545  0.446876   \n",
       "4     0.633559 -0.084937  0.473188  ...  0.027544  0.642669  0.397099   \n",
       "...        ...       ...       ...  ...       ...       ...       ...   \n",
       "2635  0.663581 -0.071532  0.455022  ...  0.004841  0.630023  0.445232   \n",
       "2636  0.640948 -0.070059  0.453466  ... -0.009315  0.629221  0.444486   \n",
       "2637  0.646457 -0.066818  0.467872  ...  0.003427  0.629216  0.390856   \n",
       "2638  0.612962 -0.069593  0.482207  ...  0.029519  0.633312  0.397255   \n",
       "2639  0.670686 -0.057918  0.502943  ...  0.010694  0.615175  0.439883   \n",
       "\n",
       "      lm_476_z  lm_477_x  lm_477_y  lm_477_z  lm_478_x  lm_478_y  lm_478_z  \n",
       "0    -0.012127  0.600117  0.469423 -0.012155  0.626503  0.492920 -0.012152  \n",
       "1    -0.018649  0.613579  0.387337 -0.018672  0.633811  0.402824 -0.018671  \n",
       "2     0.034502  0.589599  0.468398  0.034472  0.615148  0.488124  0.034479  \n",
       "3    -0.005354  0.593047  0.469114 -0.005385  0.618375  0.489911 -0.005381  \n",
       "4     0.027547  0.614630  0.422473  0.027513  0.642357  0.448079  0.027519  \n",
       "...        ...       ...       ...       ...       ...       ...       ...  \n",
       "2635  0.004842  0.604482  0.466323  0.004816  0.629380  0.487871  0.004818  \n",
       "2636 -0.009318  0.606545  0.462531 -0.009343  0.627597  0.482280 -0.009339  \n",
       "2637  0.003430  0.601153  0.416639  0.003394  0.629049  0.442369  0.003398  \n",
       "2638  0.029521  0.608004  0.419239  0.029489  0.632230  0.443043  0.029497  \n",
       "2639  0.010695  0.590612  0.463593  0.010662  0.615574  0.485904  0.010667  \n",
       "\n",
       "[2640 rows x 1434 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af04a533",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         happy\n",
       "1         anger\n",
       "2         happy\n",
       "3         happy\n",
       "4           sad\n",
       "         ...   \n",
       "2635      happy\n",
       "2636      happy\n",
       "2637    neutral\n",
       "2638      anger\n",
       "2639      happy\n",
       "Name: label, Length: 2640, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "63ddcc62",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 1., 0., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.],\n",
       "       ...,\n",
       "       [0., 0., 1., 0.],\n",
       "       [1., 0., 0., 0.],\n",
       "       [0., 1., 0., 0.]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#Get the labels\n",
    "encoder = OneHotEncoder()\n",
    "y_train = encoder.fit_transform(np.array(y_train).reshape(-1,1)).toarray()\n",
    "y_test = encoder.fit_transform(np.array(y_test).reshape(-1,1)).toarray()\n",
    "\n",
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bb9dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#scaler = StandardScaler()\n",
    "#x_train = scaler.fit_transform(x_train)\n",
    "#x_test = scaler.transform(x_test)\n",
    "# x_train.shape, y_train.shape, x_test.shape, y_test.shape\n",
    "\n",
    "#x_train = np.expand_dims(x_train, axis=2)\n",
    "#x_test = np.expand_dims(x_test, axis=2)\n",
    "# x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86f45f48",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "x_train.shape, y_train.shape, x_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d28fc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential()\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu', input_shape=(x_train.shape[1], 1)))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "model.add(Conv1D(256, kernel_size=5, strides=1, padding='same', activation='relu'))\n",
    "model.add(MaxPooling1D(pool_size=5, strides = 2, padding = 'same'))\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=32, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "\n",
    "model.add(Dense(units=4, activation='softmax'))\n",
    "model.compile(optimizer = 'adam' , loss = 'categorical_crossentropy' , metrics = ['accuracy'])\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967196a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_filepath = os.getcwd() + '/ckpt_cnn/Epoch{epoch:02d}_{accuracy:.2f}.keras'\n",
    "checkpoint_filepath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "163a0cf4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_checkpoint_callback = keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_filepath,\n",
    "    monitor='accuracy',\n",
    "    mode='max')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fc1e5c8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rlrp = ReduceLROnPlateau(monitor='accuracy', factor=0.8, verbose=1, patience=2, min_lr=0.0001)\n",
    "history = model.fit(x_train, y_train, batch_size=64, epochs=30, validation_data=(x_test, y_test), callbacks=[rlrp, model_checkpoint_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d5e578",
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = [i for i in range(30)]\n",
    "fig, ax = plt.subplots(1,2)\n",
    "train_accuracy = history.history['accuracy']\n",
    "train_loss = history.history['loss']\n",
    "test_accuracy = history.history['val_accuracy']\n",
    "test_loss = history.history['val_loss']\n",
    "\n",
    "fig.set_size_inches(20,6)\n",
    "ax[0].plot(epochs, train_loss, label = 'Training Loss')\n",
    "ax[0].plot(epochs, test_loss, label = 'Validation Loss')\n",
    "ax[0].set_title('Training & Validation Loss Over Epochs')\n",
    "ax[0].legend()\n",
    "ax[0].set_xlabel(\"Epochs\")\n",
    "\n",
    "ax[1].plot(epochs, train_accuracy, label = 'Training Accuracy')\n",
    "ax[1].plot(epochs, test_accuracy, label = 'Validation Accuracy')\n",
    "ax[1].set_title('Training & Validation Accuracy Over Epochs')\n",
    "ax[1].legend()\n",
    "ax[1].set_xlabel(\"Epochs\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efbc5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_test = model.predict(x_test)\n",
    "y_pred = encoder.inverse_transform(pred_test)\n",
    "y_test = encoder.inverse_transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f622c34f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7ab1a4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred)\n",
    "plt.figure(figsize = (12, 10))\n",
    "cm = pd.DataFrame(cm , index = [i for i in encoder.categories_] , columns = [i for i in encoder.categories_])\n",
    "sns.heatmap(cm, linecolor='white', cmap='Blues', linewidth=1, annot=True, fmt='')\n",
    "plt.title('Confusion Matrix', size=20)\n",
    "plt.xlabel('Predicted Labels', size=14)\n",
    "plt.ylabel('Actual Labels', size=14)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83c28ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "acs_test = accuracy_score(y_test,y_pred)\n",
    "ps_test = precision_score(y_test,y_pred, average = \"macro\")\n",
    "rs_test = recall_score(y_test,y_pred,  average = \"macro\")\n",
    "fs_test = f1_score(y_test,y_pred,  average = \"macro\")\n",
    "\n",
    "print(\"Accuracy Score: \", \"{:.2%}\".format(acs_test))\n",
    "print(\"Precision Score: \", \"{:.2%}\".format(ps_test))\n",
    "print(\"Recall Score: \", \"{:.2%}\".format(rs_test))\n",
    "print(\"F1 Score: \", \"{:.2%}\".format(fs_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6e01d89",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
