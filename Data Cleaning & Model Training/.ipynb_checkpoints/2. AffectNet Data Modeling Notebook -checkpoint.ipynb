{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "af3bbc6efe5548ec948499eb9a744745",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Recognition of Affective State Through Facial Expressions \n",
    "## Members\n",
    "- DICHOSO, Aaron Gabrielle C.\n",
    "- NATIVIDAD, Josh Austin Mikhail T.\n",
    "- RAZON, Luis Miguel Antonio B."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "247d7627e8ac47de86609199994dd54b",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# AffectNet\n",
    "**Dataset Description** (from [link](https://paperswithcode.com/dataset/affectnet))\\\n",
    "AffectNet is a large facial expression dataset with around 0.4 million images manually labeled for the presence of eight (neutral, happy, angry, sad, fear, surprise, disgust, contempt) facial expressions along with the intensity of valence and arousal.\n",
    "\n",
    "This dataset retrieved was already pre-processed by [Noam Segal](https://www.kaggle.com/noamsegal) for the purpose of machine learning. All images were reduced to 96 x 96 pixels, and cropped to the face. Some monochromatic images were removed through the use of Principal Component Analysis.\n",
    "\n",
    "**Notebook Description**\\\n",
    "This notebook utilizes the AffectNet Dataset to create a model focused on the recognition of four emotions (happy, sad, angry, and neutral) based on the given facial expression features extracted by the OpenCV library."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "17939f45426b4690b14d83ee475b9a28",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# Libraries\n",
    "These libraries were used in the development of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: opencv-python in c:\\users\\aaron\\anaconda3\\lib\\site-packages (4.9.0.80)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from opencv-python) (1.23.5)\n",
      "Requirement already satisfied: pip in c:\\users\\aaron\\anaconda3\\lib\\site-packages (22.3.1)\n",
      "Collecting pip\n",
      "  Using cached pip-24.0-py3-none-any.whl (2.1 MB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: To modify pip, please run the following command:\n",
      "C:\\Users\\Aaron\\anaconda3\\python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: mtcnn in c:\\users\\aaron\\anaconda3\\lib\\site-packages (0.1.1)\n",
      "Requirement already satisfied: opencv-python>=4.1.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mtcnn) (4.9.0.80)\n",
      "Requirement already satisfied: keras>=2.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mtcnn) (2.15.0)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from opencv-python>=4.1.0->mtcnn) (1.23.5)\n",
      "Requirement already satisfied: mediapipe in c:\\users\\aaron\\anaconda3\\lib\\site-packages (0.10.11)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (3.7.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (1.23.5)\n",
      "Requirement already satisfied: attrs>=19.1.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (22.1.0)\n",
      "Requirement already satisfied: opencv-contrib-python in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (4.9.0.80)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (23.5.26)\n",
      "Requirement already satisfied: protobuf<4,>=3.11 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (3.20.3)\n",
      "Requirement already satisfied: absl-py in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (2.1.0)\n",
      "Requirement already satisfied: jax in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.25)\n",
      "Requirement already satisfied: sounddevice>=0.4.4 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from mediapipe) (0.4.6)\n",
      "Requirement already satisfied: CFFI>=1.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from sounddevice>=0.4.4->mediapipe) (1.15.1)\n",
      "Requirement already satisfied: ml-dtypes>=0.2.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from jax->mediapipe) (0.2.0)\n",
      "Requirement already satisfied: opt-einsum in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from jax->mediapipe) (3.3.0)\n",
      "Requirement already satisfied: scipy>=1.9 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from jax->mediapipe) (1.10.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (2.8.2)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (22.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (9.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (3.0.9)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (4.25.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.4.4)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (0.11.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from matplotlib->mediapipe) (1.0.5)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe) (2.21)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib->mediapipe) (1.16.0)\n",
      "Requirement already satisfied: tensorflow in c:\\users\\aaron\\anaconda3\\lib\\site-packages (2.15.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.15.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow) (2.15.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (4.4.0)\n",
      "Requirement already satisfied: ml-dtypes~=0.2.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.31.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (16.0.6)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (23.5.26)\n",
      "Requirement already satisfied: packaging in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (22.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: wrapt<1.15,>=1.11.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.14.1)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: setuptools in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (65.6.3)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.62.0)\n",
      "Requirement already satisfied: h5py>=2.9.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.7.0)\n",
      "Requirement already satisfied: tensorboard<2.16,>=2.15 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (1.23.5)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (3.20.3)\n",
      "Requirement already satisfied: keras<2.16,>=2.15.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorflow-intel==2.15.0->tensorflow) (2.15.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.15.0->tensorflow) (0.38.4)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.28.1)\n",
      "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.2.0)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.2.2)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.2.8)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (5.3.3)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (4.9)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (1.26.14)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.4)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.0.4)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow-intel==2.15.0->tensorflow) (3.2.2)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\aaron\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from pandas) (2022.7)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: numpy>=1.21.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from pandas) (1.23.5)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\aaron\\anaconda3\\lib\\site-packages (1.2.1)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-learn) (1.23.5)\n",
      "Requirement already satisfied: scipy>=1.3.2 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.0)\n",
      "Requirement already satisfied: joblib>=1.1.1 in c:\\users\\aaron\\anaconda3\\lib\\site-packages (from scikit-learn) (1.1.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n"
     ]
    }
   ],
   "source": [
    "# If you are accessing this notebook for the first time without the necessary libraries, run this code block to download them.\n",
    "\n",
    "!pip install opencv-python\n",
    "!pip install --upgrade pip\n",
    "!pip install mtcnn\n",
    "!pip install mediapipe\n",
    "!pip install tensorflow\n",
    "!pip install pandas\n",
    "!pip install scikit-learn\n",
    "!wget -O face_landmarker_v2_with_blendshapes.task -q https://storage.googleapis.com/mediapipe-models/face_landmarker/face_landmarker/float16/1/face_landmarker.task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cell_id": "7c53472973744fd8a96b2a39641ad392",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 49903,
    "execution_start": 1707408747086,
    "source_hash": null
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Aaron\\anaconda3\\lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "import cv2\n",
    "import os\n",
    "import math\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import statistics\n",
    "from mtcnn import MTCNN\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "953a60b9fe544bfa94007c74f4ef23a2",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# 1. Data Import and Visualization\n",
    "\n",
    "The file `cleaned_faces.csv` contain the cleaned data from the AffectNet dataset. Execution of this notebook **requires** that the previous notebook: `1. AffectNet Data Cleaning Notebook` be executed all the way to the end first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cell_id": "1bea2d9953aa472482aff39e92ea8546",
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 5634,
     "pageSize": 5,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": false,
    "execution_millis": 495,
    "execution_start": 1707408796998,
    "source_hash": null
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>pth</th>\n",
       "      <th>label</th>\n",
       "      <th>relFCs</th>\n",
       "      <th>faceDetected</th>\n",
       "      <th>numFaces</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>anger/image0000060.jpg</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.852311</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4</td>\n",
       "      <td>anger/image0000106.jpg</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.849108</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>5</td>\n",
       "      <td>anger/image0000132.jpg</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.819448</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6</td>\n",
       "      <td>anger/image0000138.jpg</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.852052</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8</td>\n",
       "      <td>anger/image0000195.jpg</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.823133</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9101</th>\n",
       "      <td>28135</td>\n",
       "      <td>surprise/image0034829.jpg</td>\n",
       "      <td>happy</td>\n",
       "      <td>0.807797</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9102</th>\n",
       "      <td>28136</td>\n",
       "      <td>surprise/image0034831.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.839804</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>28164</td>\n",
       "      <td>surprise/image0034931.jpg</td>\n",
       "      <td>happy</td>\n",
       "      <td>0.872834</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9104</th>\n",
       "      <td>28165</td>\n",
       "      <td>surprise/image0034939.jpg</td>\n",
       "      <td>neutral</td>\n",
       "      <td>0.882535</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9105</th>\n",
       "      <td>28166</td>\n",
       "      <td>surprise/image0034946.jpg</td>\n",
       "      <td>happy</td>\n",
       "      <td>0.794207</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>9106 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Unnamed: 0                        pth    label    relFCs  faceDetected  \\\n",
       "0              1     anger/image0000060.jpg    anger  0.852311          True   \n",
       "1              4     anger/image0000106.jpg    anger  0.849108          True   \n",
       "2              5     anger/image0000132.jpg    anger  0.819448          True   \n",
       "3              6     anger/image0000138.jpg    anger  0.852052          True   \n",
       "4              8     anger/image0000195.jpg    anger  0.823133          True   \n",
       "...          ...                        ...      ...       ...           ...   \n",
       "9101       28135  surprise/image0034829.jpg    happy  0.807797          True   \n",
       "9102       28136  surprise/image0034831.jpg  neutral  0.839804          True   \n",
       "9103       28164  surprise/image0034931.jpg    happy  0.872834          True   \n",
       "9104       28165  surprise/image0034939.jpg  neutral  0.882535          True   \n",
       "9105       28166  surprise/image0034946.jpg    happy  0.794207          True   \n",
       "\n",
       "      numFaces  \n",
       "0            1  \n",
       "1            1  \n",
       "2            1  \n",
       "3            1  \n",
       "4            1  \n",
       "...        ...  \n",
       "9101         1  \n",
       "9102         1  \n",
       "9103         1  \n",
       "9104         1  \n",
       "9105         1  \n",
       "\n",
       "[9106 rows x 6 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cleaned_faces_df = pd.read_csv('outputs/cleaned_faces.csv')\n",
    "cleaned_faces_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "54bc80619da64a6880999a46a6c43d49",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "# 2. Data Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Extracting Facial Landmarks\n",
    "\n",
    "After the dataset has been processed, it is now possible to extract the facial landmarks that will be used by a machine learning model for the prediction of the given emotion labels.\n",
    "\n",
    "Different machine learning models will be utilized to test different approaches in terms of speed and performance in classification problems.\n",
    "\n",
    "### 1st Model Approach: [Mediapipe Facial Landmark Detection Model](https://developers.google.com/mediapipe/solutions/vision/face_landmarker)\n",
    "The Mediapipe Face Landmarker uses machine learning models to create a face mesh that outputs an estimate of 478 3-dimensional face landmarks. The large amount of facial landmarks generated by this model serve as rich representations of the facial expressions. It is able to capture the key points and positions of each part of the face, such as the eyebrows, eyes, nose, and mouth, and even the small details such as the forehead, cheeks, and jaw, allowing the model to provide a comprehensive analysis of facial expressions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Facial Landmark Model Testing\n",
    "\n",
    "Before extracting the facial landmarks of every face in the dataset, the model was first tested on an image not part of the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 19\u001b[0m\n\u001b[0;32m     16\u001b[0m image \u001b[38;5;241m=\u001b[39m mp\u001b[38;5;241m.\u001b[39mImage\u001b[38;5;241m.\u001b[39mcreate_from_file(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./mediapipe_test.jpg\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     17\u001b[0m detection_result \u001b[38;5;241m=\u001b[39m detector2\u001b[38;5;241m.\u001b[39mdetect(image) \u001b[38;5;66;03m#this is the line\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m annotated_image \u001b[38;5;241m=\u001b[39m \u001b[43mdraw_landmarks_on_image\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnumpy_view\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdetection_result\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28mprint\u001b[39m(detection_result)\n\u001b[0;32m     21\u001b[0m \u001b[38;5;66;03m#original image\u001b[39;00m\n",
      "File \u001b[1;32m~\\Desktop\\Github Projects\\AFFECT\\AFFECTV-XX22-Project-revised-oversampling\\mediapipe_modeling.py:6\u001b[0m, in \u001b[0;36mdraw_landmarks_on_image\u001b[1;34m(rgb_image, detection_result)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdraw_landmarks_on_image\u001b[39m(rgb_image, detection_result):\n\u001b[0;32m      5\u001b[0m   face_landmarks_list \u001b[38;5;241m=\u001b[39m detection_result\u001b[38;5;241m.\u001b[39mface_landmarks\n\u001b[1;32m----> 6\u001b[0m   annotated_image \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241m.\u001b[39mcopy(rgb_image)\n\u001b[0;32m      8\u001b[0m   \u001b[38;5;66;03m# Loop through the detected faces to visualize.\u001b[39;00m\n\u001b[0;32m      9\u001b[0m   \u001b[38;5;28;01mfor\u001b[39;00m idx \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(face_landmarks_list)):\n",
      "\u001b[1;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "from mediapipe_modeling import draw_landmarks_on_image\n",
    "#Model Setup\n",
    "BaseOptions = mp.tasks.BaseOptions\n",
    "FaceLandmarker = mp.tasks.vision.FaceLandmarker\n",
    "FaceLandmarkerOptions = mp.tasks.vision.FaceLandmarkerOptions\n",
    "VisionRunningMode = mp.tasks.vision.RunningMode\n",
    "\n",
    "base_options = python.BaseOptions(model_asset_path='face_landmarker_v2_with_blendshapes.task')\n",
    "options = vision.FaceLandmarkerOptions(base_options=base_options,\n",
    "                                       output_face_blendshapes=True,\n",
    "                                       output_facial_transformation_matrixes=True,\n",
    "                                       num_faces=1)\n",
    "\n",
    "#Test the model\n",
    "detector2 = vision.FaceLandmarker.create_from_options(options)               \n",
    "image = mp.Image.create_from_file(\"./mediapipe_test.jpg\")\n",
    "detection_result = detector2.detect(image) #this is the line\n",
    "\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "print(detection_result)\n",
    "#original image\n",
    "plt.figure()\n",
    "plt.imshow(image.numpy_view())\n",
    "plt.title('Original image')\n",
    "plt.axis('off')\n",
    "#annotated image\n",
    "plt.figure()\n",
    "plt.imshow(annotated_image)\n",
    "plt.title('MediaPipe face landmarks')\n",
    "plt.axis('off')\n",
    "\n",
    "#display the resulting coordinates of each landmark\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(\"Obtained Landmarks\")\n",
    "print(detection_result.face_landmarks[0][0].x, detection_result.face_landmarks[0][0].y, detection_result.face_landmarks[0][0].z)\n",
    "print('Count: ', len(detection_result.face_landmarks[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Appending Facial Landmark Values in `cleaned_faces_df`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "i = 0\n",
    "# add a 'landmarks' column to the dataframe\n",
    "cleaned_faces_df['landmarks'] = None\n",
    "for idx, row in cleaned_faces_df.iterrows():\n",
    "    face_img = mp.Image.create_from_file(base_path + row['pth'])\n",
    "    detection_result = detector2.detect(face_img)\n",
    "    \n",
    "    #if no face detected, drop the row\n",
    "    if len(detection_result.face_landmarks) <= 0:\n",
    "        cleaned_faces_df.drop(index=idx, inplace=True)\n",
    "        continue\n",
    "    #turn it into a list of tuples with the x, y, z coordinates\n",
    "    cleaned_faces_df.at[idx, 'landmarks'] = [ (landmark.x, landmark.y, landmark.z) for landmark in detection_result.face_landmarks[0] ]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_faces_df.to_csv('outputs\\cleaned_faces_with_landmarks_mediapipe.csv', index=False, header=True, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optional: Importing\n",
    "use the code block below to reimport data of faces with facial landmarks extracted, as long as no changes were performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "landmarks_faces_df = pd.read_csv('outputs\\cleaned_faces_with_landmarks_mediapipe.csv')\n",
    "#parse the landmarks column to a list of tuples\n",
    "landmarks_faces_df['landmarks'] = landmarks_faces_df['landmarks'].apply(eval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"adressing-label-mismatch\"></a>\n",
    "\n",
    "## 2.4 Addressing label mismatch\n",
    "As discussed previously, label mismatches were found in the dataset wherein the emotion present in the `label` attribute does not match the emotion presented in the `pth` attribute. To address this, we will duplicate the mismatched rows so that one row will contain the emotion present in the `pth` attribute in the `label` attribute, and the other row will contain the emotion present in the `label` attribute in the `label` attribute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check mismatched labels again after cleaning\n",
    "mismatch_df = landmarks_faces_df.loc[landmarks_faces_df['pth'].str.split('/').str[0] != landmarks_faces_df['label']]\n",
    "\n",
    "print(f\"Mismatched Path-Label: {mismatch_df.shape[0]}\")\n",
    "print(f\"Before: {landmarks_faces_df.shape[0]}\")\n",
    "for idx, row in mismatch_df.iterrows():\n",
    "    pth_emotion = row['pth'].split('/')[0]\n",
    "\n",
    "    #find the row in landmarks_faces_df and deep copy\n",
    "    cleaned_faces_row = landmarks_faces_df.loc[landmarks_faces_df['pth'] == row['pth']].copy(deep=True)\n",
    "    cleaned_faces_row['label'] = pth_emotion\n",
    "\n",
    "    #If the row is present, concat the new row with the changed value\n",
    "    if cleaned_faces_row.shape[0] > 0:\n",
    "        landmarks_faces_df = pd.concat([landmarks_faces_df, cleaned_faces_row])\n",
    "        \n",
    "emotions = ['happy', 'sad', 'anger', 'neutral']\n",
    "landmarks_faces_df = landmarks_faces_df[landmarks_faces_df['label'].isin(emotions)]\n",
    "print(f\"After: {landmarks_faces_df.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displays the rows with duplicate paths to validate code above works\n",
    "landmarks_faces_df[landmarks_faces_df.duplicated(subset='pth', keep=False)].sort_values(by='pth').head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5 Dataset splitting\n",
    "Before we perform oversampling, we must first split the dataset into train and test sets for training and validating the models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_df = landmarks_faces_df[['label','landmarks']]\n",
    "model_training_df.reset_index(inplace=True, drop=True)\n",
    "model_training_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract x, y, and z coordinates from the tuples in the 'landmarks' column\n",
    "coordinates = model_training_df['landmarks'].apply(lambda x: [coord for landmark in x for coord in landmark])\n",
    "\n",
    "# Create a DataFrame from the extracted coordinates\n",
    "landmarks_df = pd.DataFrame(coordinates.tolist(), columns=[f'lm_{i+1}_{coord}' for i in range(len(coordinates.iloc[0]) // 3) for coord in ['x', 'y', 'z']])\n",
    "\n",
    "# Concatenate the original DataFrame with the new landmarks DataFrame\n",
    "model_training_df = pd.concat([model_training_df, landmarks_df], axis=1)\n",
    "\n",
    "# Drop the original 'landmarks' column\n",
    "model_training_df.drop(columns=['landmarks'], inplace=True)\n",
    "model_training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Landmarks data as features to be used\n",
    "x = model_training_df.iloc[:, 1:]\n",
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Emotion labels\n",
    "y = model_training_df['label']\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Splitting of data into train and test sets, \n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, shuffle=True, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Combine Train set and Test set for extraction and oversampling\n",
    "train_set = pd.concat([y_train, x_train], axis = 1)\n",
    "train_set.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.concat([y_test, x_test], axis = 1)\n",
    "test_set.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "cell_id": "aa208e55c899487c8ac01355dce5a3e8",
    "deepnote_cell_type": "markdown"
   },
   "source": [
    "## 2.6 Oversampling\n",
    "This will be used to oversample images according to the highest amount of images in an emotion label. This is needed so that during the training portion of the emotion recognition model, it's emotion recognition functionality may not be skewed favoring one emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "ba47acd37a634b318a3fab259ec4b9a5",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 68,
    "execution_start": 1707297995717,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "dict_df_emotions = {\n",
    "    \"happy\" :  train_set[train_set['label'] == 'happy'],\n",
    "    \"sad\" : train_set[train_set['label'] == 'sad'],\n",
    "    \"anger\" : train_set[train_set['label'] == 'anger'],\n",
    "    \"neutral\" : train_set[train_set['label'] == 'neutral']\n",
    "}\n",
    "\n",
    "dict_counts = {\n",
    "    \"happy\" : dict_df_emotions['happy'].shape[0],\n",
    "    \"sad\" : dict_df_emotions['sad'].shape[0],\n",
    "    \"anger\" : dict_df_emotions['anger'].shape[0],\n",
    "    \"neutral\" : dict_df_emotions['neutral'].shape[0]\n",
    "}\n",
    "\n",
    "dict_counts = dict(sorted(dict_counts.items(), key = lambda item : item[1],  reverse = True))\n",
    "dict_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "2e07bdf4b1e1407099e457ac2546b707",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 23,
    "execution_start": 1707298004898,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "#Step 3: Get number of oversampled images needed for each emotion\n",
    "highest_count = dict_counts[list(dict_counts)[0]]\n",
    "\n",
    "print(highest_count)\n",
    "\n",
    "oversampling_count = {\n",
    "    \"happy\" : highest_count - dict_df_emotions['happy'].shape[0],\n",
    "    \"sad\" : highest_count - dict_df_emotions['sad'].shape[0],\n",
    "    \"anger\" : highest_count - dict_df_emotions['anger'].shape[0],\n",
    "    \"neutral\" : highest_count - dict_df_emotions['neutral'].shape[0]\n",
    "}\n",
    "\n",
    "oversampling_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "abe2b3ffd8794114ba40ea17ce140ce9",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 92,
    "execution_start": 1707298046441,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "#Step 4: for each emotion, generate new images according to oversampling count\n",
    "\n",
    "for emotion in list(oversampling_count):\n",
    "    print(\"=======!!!\", emotion, oversampling_count[emotion],\"!!!=======\")\n",
    "    max_range = dict_df_emotions[emotion].shape[0] - 1\n",
    "    added_rows_df = pd.DataFrame()\n",
    "    emotion_indices = []\n",
    "    print(dict_df_emotions[emotion])\n",
    "    print(\"====== INDEX:\", max_range,\"======\")\n",
    "    print(dict_df_emotions[emotion].iloc[max_range])\n",
    "    for i in range(oversampling_count[emotion]):\n",
    "        #Pick a random entry inside of the given emotion\n",
    "        random_index = randint(0, max_range)\n",
    "        \n",
    "        emotion_indices.append(random_index)\n",
    "    \n",
    "    added_rows_df = pd.concat([added_rows_df, dict_df_emotions[emotion].iloc[emotion_indices]])\n",
    "    \n",
    "    print(\"Count:\", added_rows_df.shape[0])\n",
    "    \n",
    "    dict_df_emotions[emotion] = pd.concat([dict_df_emotions[emotion], added_rows_df])\n",
    "    \n",
    "    print(\"New Count:\", dict_df_emotions[emotion].shape[0])\n",
    "    \n",
    "    added_rows_df.drop(added_rows_df.index , inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "1b92e46059a64381a29c3a8caa477a10",
    "deepnote_cell_type": "code",
    "deepnote_table_loading": false,
    "deepnote_table_state": {
     "filters": [],
     "pageIndex": 1,
     "pageSize": 10,
     "sortBy": []
    },
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 105,
    "execution_start": 1707298928122,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "#Last step: merge all emotions back\n",
    "dfs = [dict_df_emotions['happy'], dict_df_emotions['sad'], dict_df_emotions['anger'], dict_df_emotions['neutral']]\n",
    "new_faces_df = pd.concat(dfs)\n",
    "new_faces_df\n",
    "\n",
    "#remove \"Unnamed: 0\" column and reset index\n",
    "new_faces_df.reset_index(drop=True, inplace=True)\n",
    "new_faces_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cell_id": "6a994e9c282547d6a606bb3af81bde91",
    "deepnote_cell_type": "code",
    "deepnote_to_be_reexecuted": true,
    "execution_millis": 107,
    "execution_start": 1707299168521,
    "source_hash": null
   },
   "outputs": [],
   "source": [
    "sum_df = new_faces_df.groupby(['label']).count()\n",
    "plt.pie(sum_df['landmark_1_x'], labels = ['Anger', 'Happy', 'Neutral', 'Sad'])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Exporting Datasets to CSV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we export the train and test datasets to be used in other notebooks that would use the datasets for implementing differnet models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exporting Datasets\n",
    "train_set\n",
    "train_set.to_csv('outputs/train_set.csv', index=False, header=True, encoding='utf-8')\n",
    "test_set.to_csv('outputs/test_set.csv', index=False, header=True, encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "deepnote_execution_queue": [],
  "deepnote_notebook_id": "6eaf694bc14f43a6a60b6021c3ee9ad8",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
